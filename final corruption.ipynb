{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c079b53f-ddb5-41c3-8f98-df51fa13e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93be04d0-a475-4bad-b5ca-6b6d906c3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(\"opinions.csv\")\n",
    "with open(\"opinions.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    case=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda403b4-1f3a-44ea-8ea6-630acdf8e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"it'll\", \"they're\", 'between', 'himself', 'same', \"we'll\", 'only', 'further', 'such', 'his', 'isn', \"i've\", 'ain', \"he's\", \"mightn't\", \"he'll\", 'once', 'below', 'll', 'again', 'has', 'more', 'on', 'hers', 'herself', 'yours', 'which', \"it's\", 'own', 'what', 'her', 'your', \"aren't\", 'out', 'during', 'why', 'were', 'don', 'doesn', 'for', 'there', \"won't\", 'm', \"don't\", 'nor', 'each', 'are', 're', \"should've\", \"weren't\", \"mustn't\", 'd', 'over', 'because', 'o', \"she'll\", 'having', 'my', 'y', 'me', 'who', 'didn', 'into', 'he', 'of', 'at', \"they'll\", \"it'd\", 'very', 'too', \"we'd\", 'we', 'not', 'just', 'wouldn', 'from', \"shan't\", 'then', 'no', 'so', 'ma', \"needn't\", \"doesn't\", 'be', 'aren', 'down', \"you'd\", \"i'm\", 'any', \"didn't\", 'up', 'to', 'it', \"isn't\", 'this', 'being', \"hasn't\", 'other', 'now', 'these', 'ours', \"hadn't\", \"haven't\", \"they've\", \"wasn't\", 'until', 'myself', 'by', 'you', 'about', 'than', 'wasn', 'am', \"they'd\", 'do', 'but', 'itself', 'the', 'theirs', \"you've\", \"couldn't\", 'have', 'against', \"we're\", 'if', 'when', 'couldn', 'here', 'shouldn', 'with', 'mightn', 'our', 'how', 'off', 'does', \"she'd\", 'hadn', 'is', 'shan', 'will', 'or', 'yourself', 'both', 'most', 'themselves', \"that'll\", 'needn', 'through', 'its', 'a', 'their', 'some', 'weren', 'in', 'had', 'while', 'mustn', \"i'd\", 'hasn', 'haven', 'ourselves', 'them', 've', \"we've\", \"shouldn't\", 'those', 'should', 'above', 'all', 'can', \"i'll\", 'i', \"wouldn't\", 'before', 'after', 'where', \"you'll\", \"he'd\", 'been', 'him', 'doing', 'under', 'yourselves', 'an', 'as', 'she', 'that', 'won', 'and', 'they', \"you're\", 'did', \"she's\", 't', 'was', 's', 'few', 'whom']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words_list=list(stop_words)\n",
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4881b38f-c596-46e3-96e7-0eaa7dc2a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'author_name,category,per_curiam,case_name,date_filed,federal_cite_one,absolute_url,cluster,year_filed,scdb_id,scdb_decision_direction,scdb_votes_majority,scdb_votes_minority,text\\nJustice Roberts,majority,False,McCutcheon v. Federal Election Comm\\'n,2014-04-02,,https://www.courtlistener.com/opinion/2659301/mccutcheon-v-federal-election-commn/,https://www.courtlistener.com/api/rest/v3/clusters/2659301/,2014,2013-033,1.0,5.0,4.0,\"There is no right more basic in our democracy than the\\nright to participate in electing our political leaders. Citi-\\nzens can exercise that right in a variety of ways: They can\\nrun for office themselves, vote, urge others to vote for a\\nparticular candidate, volunteer to work on a campaign,\\nand contribute to a candidate’s campaign. This case is\\nabout the last of those options.\\n   The right to participate in democracy through political\\ncontributions is protected by the First Amendment, but\\nthat right is not absolute. Our cases have held that Con-\\ngress may regulate '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bir bölümü\n",
    "case[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2f0c5f-492b-45c0-9fa7-837e59772e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize ve stemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stemmer=PorterStemmer()\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c28c3dc-09eb-4d07-9897-bf3eea822392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d17a86-5c66-40c1-881f-d7345fc0cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    filtered_tokens=[token.lower() for token in tokens if token.isalpha() and token.lower not in stop_words]\n",
    "\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517c4700-61f7-4b1d-9bd8-b094e5f368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#her cünleyi tokenleştir ve lemmatize et ve stemmle\n",
    "tokenized_corpus_lemmatized=[]\n",
    "tokenized_corpus_stemmed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e518f1b3-6467-401b-b34b-9e2895b54cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens=preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f93bfe-6256-4b3f-8391-9aeb40184af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemmatized_sente.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b6e360-48e0-4311-a667-2e44223fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stemmed_sente.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b490ee11-44d9-427e-8b5b-ae228f1dd808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Base: author_name,category,per_curiam,case_name,date_filed,federal_cite_one,absolute_url,cluster,year_filed,scdb_id,scdb_decision_direction,scdb_votes_majority,scdb_votes_minority,text\n",
      "Justice Roberts,majority,False,McCutcheon v. Federal Election Comm'n,2014-04-02,,https://www.courtlistener.com/opinion/2659301/mccutcheon-v-federal-election-commn/,https://www.courtlistener.com/api/rest/v3/clusters/2659301/,2014,2013-033,1.0,5.0,4.0,\"There is no right more basic in our democracy than the\n",
      "right to participate in electing our political leaders.\n",
      "Cümle 1 - Lemmatized: ['category', 'cluster', 'text', 'justice', 'robert', 'majority', 'false', 'mccutcheon', 'federal', 'election', 'http', 'there', 'is', 'no', 'right', 'more', 'basic', 'in', 'our', 'democracy', 'than', 'the', 'right', 'to', 'participate', 'in', 'electing', 'our', 'political', 'leader']\n",
      "Cümle 1 - Stemmed: ['categori', 'cluster', 'text', 'justic', 'robert', 'major', 'fals', 'mccutcheon', 'feder', 'elect', 'http', 'there', 'is', 'no', 'right', 'more', 'basic', 'in', 'our', 'democraci', 'than', 'the', 'right', 'to', 'particip', 'in', 'elect', 'our', 'polit', 'leader']\n",
      "\n",
      "\n",
      "Cümle 2 - Base: Citi-\n",
      "zens can exercise that right in a variety of ways: They can\n",
      "run for office themselves, vote, urge others to vote for a\n",
      "particular candidate, volunteer to work on a campaign,\n",
      "and contribute to a candidate’s campaign.\n",
      "Cümle 2 - Lemmatized: ['zen', 'can', 'exercise', 'that', 'right', 'in', 'a', 'variety', 'of', 'way', 'they', 'can', 'run', 'for', 'office', 'themselves', 'vote', 'urge', 'others', 'to', 'vote', 'for', 'a', 'particular', 'candidate', 'volunteer', 'to', 'work', 'on', 'a', 'campaign', 'and', 'contribute', 'to', 'a', 'candidate', 's', 'campaign']\n",
      "Cümle 2 - Stemmed: ['zen', 'can', 'exercis', 'that', 'right', 'in', 'a', 'varieti', 'of', 'way', 'they', 'can', 'run', 'for', 'offic', 'themselv', 'vote', 'urg', 'other', 'to', 'vote', 'for', 'a', 'particular', 'candid', 'volunt', 'to', 'work', 'on', 'a', 'campaign', 'and', 'contribut', 'to', 'a', 'candid', 's', 'campaign']\n",
      "\n",
      "\n",
      "Cümle 3 - Base: This case is\n",
      "about the last of those options.\n",
      "Cümle 3 - Lemmatized: ['this', 'case', 'is', 'about', 'the', 'last', 'of', 'those', 'option']\n",
      "Cümle 3 - Stemmed: ['thi', 'case', 'is', 'about', 'the', 'last', 'of', 'those', 'option']\n",
      "\n",
      "\n",
      "Cümle 4 - Base: The right to participate in democracy through political\n",
      "contributions is protected by the First Amendment, but\n",
      "that right is not absolute.\n",
      "Cümle 4 - Lemmatized: ['the', 'right', 'to', 'participate', 'in', 'democracy', 'through', 'political', 'contribution', 'is', 'protected', 'by', 'the', 'first', 'amendment', 'but', 'that', 'right', 'is', 'not', 'absolute']\n",
      "Cümle 4 - Stemmed: ['the', 'right', 'to', 'particip', 'in', 'democraci', 'through', 'polit', 'contribut', 'is', 'protect', 'by', 'the', 'first', 'amend', 'but', 'that', 'right', 'is', 'not', 'absolut']\n",
      "\n",
      "\n",
      "Cümle 5 - Base: Our cases have held that Con-\n",
      "gress may regulate campaign contributions to protect\n",
      "against corruption or the appearance of corruption.\n",
      "Cümle 5 - Lemmatized: ['our', 'case', 'have', 'held', 'that', 'gress', 'may', 'regulate', 'campaign', 'contribution', 'to', 'protect', 'against', 'corruption', 'or', 'the', 'appearance', 'of', 'corruption']\n",
      "Cümle 5 - Stemmed: ['our', 'case', 'have', 'held', 'that', 'gress', 'may', 'regul', 'campaign', 'contribut', 'to', 'protect', 'against', 'corrupt', 'or', 'the', 'appear', 'of', 'corrupt']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ik 5 yazdıralım\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Base: {sentences[i]}\")\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9c4843-66f0-4749-93d1-094922af4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "file_path=os.path.join(\"stemmed_sente.csv\")\n",
    "with open(\"stemmed_sente.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    stemmed_texts=file.read()\n",
    "stemme_texts=[' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix=vectorizer.fit_transform(stemme_texts[:1000])\n",
    "features_names=vectorizer.get_feature_names_out()\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=features_names)\n",
    "with open(\"stemmee_idfff.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1faa19e1-8849-4a5a-8c54-77e002b34030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abid  abil  abl     about  abov  abridg  absenc  absent   absolut  abus  \\\n",
      "0   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "1   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "2   0.0   0.0  0.0  0.392252   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "3   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.331957   0.0   \n",
      "4   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "\n",
      "   ...  wrote  wyden  wyo   ye  year  yet  yield  yond  york       zen  \n",
      "0  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "1  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.222065  \n",
      "2  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "3  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "4  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "\n",
      "[5 rows x 1928 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b1be283-3146-4dde-8e7c-7c5cacd32345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilk cümlede en yüksek TF-IDF skorlarına sahip 5 kelime:\n",
      "right       0.317514\n",
      "our         0.312968\n",
      "text        0.254135\n",
      "cluster     0.254135\n",
      "categori    0.239855\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#ilk cümle için TF-IDF SKORLARINI AL\n",
    "first_sentence_vector=tfidf_df.iloc[0]\n",
    "#Skorlara göre sırala\n",
    "top_5_words=first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "#sonuç yazdır\n",
    "print(\"ilk cümlede en yüksek TF-IDF skorlarına sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "559b6cf5-a7c3-4231-9089-4b0e11dc2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts=[' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix=vectorizer.fit_transform(lemmatized_texts[:1000])\n",
    "features_names=vectorizer.get_feature_names_out()\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=features_names)\n",
    "with open(\"lemmaaa_idfff.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00198302-67d7-4171-910c-5c3458907d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abide  ability  able     about  above  abridg  abridgement  abridgment  \\\n",
      "0    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "1    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "2    0.0      0.0   0.0  0.392252    0.0     0.0          0.0         0.0   \n",
      "3    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "4    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "\n",
      "   absence  absent  ...  wrote  wyden  wyo  year  yes  yet  yield  yond  york  \\\n",
      "0      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "1      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "2      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "3      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "4      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "\n",
      "        zen  \n",
      "0  0.000000  \n",
      "1  0.215289  \n",
      "2  0.000000  \n",
      "3  0.000000  \n",
      "4  0.000000  \n",
      "\n",
      "[5 rows x 2440 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a62935a-cddc-4166-9a4f-a9cfe1351af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "parameters=[\n",
    "    {'model_type':'cbow','window':2,'vector_size':100},\n",
    "    {'model_type':'skipgram','window':2,'vector_size':100},\n",
    "    {'model_type':'cbow','window':4,'vector_size':100},\n",
    "    {'model_type':'skipgram','window':4,'vector_size':100},\n",
    "    {'model_type':'cbow','window':2,'vector_size':300},\n",
    "    {'model_type':'skipgram','window':2,'vector_size':300},\n",
    "    {'model_type':'cbow','window':4,'vector_size':300},\n",
    "    {'model_type':'skipgram','window':4,'vector_size':300}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc495df7-33e6-4c8c-a6ae-176d0b957694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100model saved\n",
      "lemmatized_model_skipgram_window2_dim100model saved\n",
      "lemmatized_model_cbow_window4_dim100model saved\n",
      "lemmatized_model_skipgram_window4_dim100model saved\n",
      "lemmatized_model_cbow_window2_dim300model saved\n",
      "lemmatized_model_skipgram_window2_dim300model saved\n",
      "lemmatized_model_cbow_window4_dim300model saved\n",
      "lemmatized_model_skipgram_window4_dim300model saved\n",
      "stemmed_model_cbow_window2_dim100model saved\n",
      "stemmed_model_skipgram_window2_dim100model saved\n",
      "stemmed_model_cbow_window4_dim100model saved\n",
      "stemmed_model_skipgram_window4_dim100model saved\n",
      "stemmed_model_cbow_window2_dim300model saved\n",
      "stemmed_model_skipgram_window2_dim300model saved\n",
      "stemmed_model_cbow_window4_dim300model saved\n",
      "stemmed_model_skipgram_window4_dim300model saved\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model=Word2Vec(corpus, vector_size=params['vector_size'], window=params['window'], min_count=1,sg=1 if params['model_type']=='skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}model saved\")\n",
    "#lemma corpus modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "#stemma corpus modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param,\"stemmed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ccacb8-8526-4e12-a375-fe7420b8f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#Model dosyalarını yüklemek\n",
    "model_1=Word2Vec.load(\"lemmatized_model_cbow_window2_dim100model\")\n",
    "model_2=Word2Vec.load(\"lemmatized_model_skipgram_window2_dim100model\")\n",
    "model_3=Word2Vec.load(\"lemmatized_model_cbow_window4_dim100model\")\n",
    "model_4=Word2Vec.load(\"lemmatized_model_skipgram_window4_dim100model\")\n",
    "model_5=Word2Vec.load(\"lemmatized_model_cbow_window2_dim300model\")\n",
    "model_6=Word2Vec.load(\"lemmatized_model_skipgram_window2_dim300model\")\n",
    "model_7=Word2Vec.load(\"lemmatized_model_cbow_window4_dim300model\")\n",
    "model_8=Word2Vec.load(\"lemmatized_model_skipgram_window4_dim300model\")\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity=model.wv.most_similar('corruption', topn=5)\n",
    "    print(f\"\\n{model_name} Modeli - 'corruption' ile benzer 5:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru:{score}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da22479-c70e-4e6c-baee-85f7a6fb45d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lemmatized_model_cbow_window2_dim100model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: deception, Benzerlik Skoru:0.773451566696167\n",
      "Kelime: partiality, Benzerlik Skoru:0.7485675811767578\n",
      "Kelime: favoritism, Benzerlik Skoru:0.734829843044281\n",
      "Kelime: intimidation, Benzerlik Skoru:0.701158344745636\n",
      "Kelime: oppression, Benzerlik Skoru:0.680753231048584\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim100model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: partiality, Benzerlik Skoru:0.7175496816635132\n",
      "Kelime: bias, Benzerlik Skoru:0.7073368430137634\n",
      "Kelime: overreaching, Benzerlik Skoru:0.7070448398590088\n",
      "Kelime: intimidation, Benzerlik Skoru:0.6958297491073608\n",
      "Kelime: deception, Benzerlik Skoru:0.6806089878082275\n",
      "\n",
      "lemmatized_model_cbow_window4_dim100model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: overreaching, Benzerlik Skoru:0.7444360852241516\n",
      "Kelime: partiality, Benzerlik Skoru:0.7325118184089661\n",
      "Kelime: lawlessness, Benzerlik Skoru:0.7183961272239685\n",
      "Kelime: deception, Benzerlik Skoru:0.715507984161377\n",
      "Kelime: oppression, Benzerlik Skoru:0.6912490725517273\n",
      "\n",
      "lemmatized_model_skipgram_window4_dim100model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: partiality, Benzerlik Skoru:0.70381760597229\n",
      "Kelime: venality, Benzerlik Skoru:0.702649712562561\n",
      "Kelime: overreaching, Benzerlik Skoru:0.6938507556915283\n",
      "Kelime: splintering, Benzerlik Skoru:0.6633927822113037\n",
      "Kelime: deception, Benzerlik Skoru:0.6561722755432129\n",
      "\n",
      "lemmatized_model_cbow_window2_dim300model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: deception, Benzerlik Skoru:0.6633692979812622\n",
      "Kelime: intimidation, Benzerlik Skoru:0.6156245470046997\n",
      "Kelime: partiality, Benzerlik Skoru:0.5965763330459595\n",
      "Kelime: overreaching, Benzerlik Skoru:0.5945219993591309\n",
      "Kelime: circumvention, Benzerlik Skoru:0.5705882906913757\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim300model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: overreaching, Benzerlik Skoru:0.5470045208930969\n",
      "Kelime: partiality, Benzerlik Skoru:0.5435625910758972\n",
      "Kelime: bribery, Benzerlik Skoru:0.5414615869522095\n",
      "Kelime: trickery, Benzerlik Skoru:0.5392423868179321\n",
      "Kelime: treachery, Benzerlik Skoru:0.5241311192512512\n",
      "\n",
      "lemmatized_model_cbow_window4_dim300model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: overreaching, Benzerlik Skoru:0.6514056921005249\n",
      "Kelime: deception, Benzerlik Skoru:0.6457278728485107\n",
      "Kelime: partiality, Benzerlik Skoru:0.6244065761566162\n",
      "Kelime: intimidation, Benzerlik Skoru:0.6212003231048584\n",
      "Kelime: bribery, Benzerlik Skoru:0.6008267402648926\n",
      "\n",
      "lemmatized_model_skipgram_window4_dim300model Modeli - 'corruption' ile benzer 5:\n",
      "Kelime: overreaching, Benzerlik Skoru:0.5345315337181091\n",
      "Kelime: bribery, Benzerlik Skoru:0.521219789981842\n",
      "Kelime: partiality, Benzerlik Skoru:0.5174458622932434\n",
      "Kelime: venality, Benzerlik Skoru:0.5056472420692444\n",
      "Kelime: malfeasance, Benzerlik Skoru:0.49770647287368774\n"
     ]
    }
   ],
   "source": [
    "#8 model için yazır\n",
    "print_similar_words(model_1, \"lemmatized_model_cbow_window2_dim100model\")\n",
    "print_similar_words(model_2, \"lemmatized_model_skipgram_window2_dim100model\")\n",
    "print_similar_words(model_3, \"lemmatized_model_cbow_window4_dim100model\")\n",
    "print_similar_words(model_4, \"lemmatized_model_skipgram_window4_dim100model\")\n",
    "print_similar_words(model_5, \"lemmatized_model_cbow_window2_dim300model\")\n",
    "print_similar_words(model_6, \"lemmatized_model_skipgram_window2_dim300model\")\n",
    "print_similar_words(model_7, \"lemmatized_model_cbow_window4_dim300model\")\n",
    "print_similar_words(model_8, \"lemmatized_model_skipgram_window4_dim300model\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d823d73-4881-4f60-b111-06e0c2bd8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#Model dosyalarını yüklemek\n",
    "mww_1=Word2Vec.load(\"stemmed_model_cbow_window2_dim100model\")\n",
    "mww_2=Word2Vec.load(\"stemmed_model_skipgram_window2_dim100model\")\n",
    "mww_3=Word2Vec.load(\"stemmed_model_cbow_window4_dim100model\")\n",
    "mww_4=Word2Vec.load(\"stemmed_model_skipgram_window4_dim100model\")\n",
    "mww_5=Word2Vec.load(\"stemmed_model_cbow_window2_dim300model\")\n",
    "mww_6=Word2Vec.load(\"stemmed_model_skipgram_window2_dim300model\")\n",
    "mww_7=Word2Vec.load(\"stemmed_model_cbow_window4_dim300model\")\n",
    "mww_8=Word2Vec.load(\"stemmed_model_skipgram_window4_dim300model\")\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity=model.wv.most_similar('corruption', topn=5)\n",
    "    print(f\"\\n{model_name} Modeli - 'corruption' ile benzer 5:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576e847d-d76d-4e97-be9d-905315cfee8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'corruption' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#8 model için yazır\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mprint_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmww_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstemmed_model_cbow_window2_dim100model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m print_similar_words(mww_2, \u001b[33m\"\u001b[39m\u001b[33mstemmed_model_skipgram_window2_dim100model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m print_similar_words(mww_3, \u001b[33m\"\u001b[39m\u001b[33mstemmed_model_cbow_window4_dim100model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mprint_similar_words\u001b[39m\u001b[34m(model, model_name)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_similar_words\u001b[39m(model, model_name):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     similarity=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcorruption\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Modeli - \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcorruption\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ile benzer 5:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word, score \u001b[38;5;129;01min\u001b[39;00m similarity:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    838\u001b[39m         weight[idx] = item[\u001b[32m1\u001b[39m]\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m mean = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m all_keys = [\n\u001b[32m    843\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_index_for(key)\n\u001b[32m    844\u001b[39m ]\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[39m, in \u001b[36mKeyedVectors.get_mean_vector\u001b[39m\u001b[34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[39m\n\u001b[32m    516\u001b[39m         total_weight += \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not present in vocabulary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_weight > \u001b[32m0\u001b[39m:\n\u001b[32m    521\u001b[39m     mean = mean / total_weight\n",
      "\u001b[31mKeyError\u001b[39m: \"Key 'corruption' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "#8 model için yazır\n",
    "print_similar_words(mww_1, \"stemmed_model_cbow_window2_dim100model\")\n",
    "print_similar_words(mww_2, \"stemmed_model_skipgram_window2_dim100model\")\n",
    "print_similar_words(mww_3, \"stemmed_model_cbow_window4_dim100model\")\n",
    "print_similar_words(mww_4, \"stemmed_model_skipgram_window4_dim100model\")\n",
    "print_similar_words(mww_5, \"stemmed_model_cbow_window2_dim300model\")\n",
    "print_similar_words(mww_6, \"stemmed_model_skipgram_window2_dim300model\")\n",
    "print_similar_words(mww_7, \"stemmed_model_cbow_window4_dim300model\")\n",
    "print_similar_words(mww_8, \"stemmed_model_skipgram_window4_dim300model\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
