{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c079b53f-ddb5-41c3-8f98-df51fa13e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93be04d0-a475-4bad-b5ca-6b6d906c3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(\"opinions.csv\")\n",
    "with open(\"opinions.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    case=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda403b4-1f3a-44ea-8ea6-630acdf8e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"it'll\", \"they're\", 'between', 'himself', 'same', \"we'll\", 'only', 'further', 'such', 'his', 'isn', \"i've\", 'ain', \"he's\", \"mightn't\", \"he'll\", 'once', 'below', 'll', 'again', 'has', 'more', 'on', 'hers', 'herself', 'yours', 'which', \"it's\", 'own', 'what', 'her', 'your', \"aren't\", 'out', 'during', 'why', 'were', 'don', 'doesn', 'for', 'there', \"won't\", 'm', \"don't\", 'nor', 'each', 'are', 're', \"should've\", \"weren't\", \"mustn't\", 'd', 'over', 'because', 'o', \"she'll\", 'having', 'my', 'y', 'me', 'who', 'didn', 'into', 'he', 'of', 'at', \"they'll\", \"it'd\", 'very', 'too', \"we'd\", 'we', 'not', 'just', 'wouldn', 'from', \"shan't\", 'then', 'no', 'so', 'ma', \"needn't\", \"doesn't\", 'be', 'aren', 'down', \"you'd\", \"i'm\", 'any', \"didn't\", 'up', 'to', 'it', \"isn't\", 'this', 'being', \"hasn't\", 'other', 'now', 'these', 'ours', \"hadn't\", \"haven't\", \"they've\", \"wasn't\", 'until', 'myself', 'by', 'you', 'about', 'than', 'wasn', 'am', \"they'd\", 'do', 'but', 'itself', 'the', 'theirs', \"you've\", \"couldn't\", 'have', 'against', \"we're\", 'if', 'when', 'couldn', 'here', 'shouldn', 'with', 'mightn', 'our', 'how', 'off', 'does', \"she'd\", 'hadn', 'is', 'shan', 'will', 'or', 'yourself', 'both', 'most', 'themselves', \"that'll\", 'needn', 'through', 'its', 'a', 'their', 'some', 'weren', 'in', 'had', 'while', 'mustn', \"i'd\", 'hasn', 'haven', 'ourselves', 'them', 've', \"we've\", \"shouldn't\", 'those', 'should', 'above', 'all', 'can', \"i'll\", 'i', \"wouldn't\", 'before', 'after', 'where', \"you'll\", \"he'd\", 'been', 'him', 'doing', 'under', 'yourselves', 'an', 'as', 'she', 'that', 'won', 'and', 'they', \"you're\", 'did', \"she's\", 't', 'was', 's', 'few', 'whom']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words_list=list(stop_words)\n",
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4881b38f-c596-46e3-96e7-0eaa7dc2a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'author_name,category,per_curiam,case_name,date_filed,federal_cite_one,absolute_url,cluster,year_filed,scdb_id,scdb_decision_direction,scdb_votes_majority,scdb_votes_minority,text\\nJustice Roberts,majority,False,McCutcheon v. Federal Election Comm\\'n,2014-04-02,,https://www.courtlistener.com/opinion/2659301/mccutcheon-v-federal-election-commn/,https://www.courtlistener.com/api/rest/v3/clusters/2659301/,2014,2013-033,1.0,5.0,4.0,\"There is no right more basic in our democracy than the\\nright to participate in electing our political leaders. Citi-\\nzens can exercise that right in a variety of ways: They can\\nrun for office themselves, vote, urge others to vote for a\\nparticular candidate, volunteer to work on a campaign,\\nand contribute to a candidate’s campaign. This case is\\nabout the last of those options.\\n   The right to participate in democracy through political\\ncontributions is protected by the First Amendment, but\\nthat right is not absolute. Our cases have held that Con-\\ngress may regulate '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bir bölümü\n",
    "case[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2f0c5f-492b-45c0-9fa7-837e59772e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize ve stemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stemmer=PorterStemmer()\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c28c3dc-09eb-4d07-9897-bf3eea822392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d17a86-5c66-40c1-881f-d7345fc0cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    filtered_tokens=[token.lower() for token in tokens if token.isalpha() and token.lower not in stop_words]\n",
    "\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517c4700-61f7-4b1d-9bd8-b094e5f368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#her cünleyi tokenleştir ve lemmatize et ve stemmle\n",
    "tokenized_corpus_lemmatized=[]\n",
    "tokenized_corpus_stemmed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e518f1b3-6467-401b-b34b-9e2895b54cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens=preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f93bfe-6256-4b3f-8391-9aeb40184af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemmatized_sente.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b6e360-48e0-4311-a667-2e44223fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stemmed_sente.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b490ee11-44d9-427e-8b5b-ae228f1dd808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Base: author_name,category,per_curiam,case_name,date_filed,federal_cite_one,absolute_url,cluster,year_filed,scdb_id,scdb_decision_direction,scdb_votes_majority,scdb_votes_minority,text\n",
      "Justice Roberts,majority,False,McCutcheon v. Federal Election Comm'n,2014-04-02,,https://www.courtlistener.com/opinion/2659301/mccutcheon-v-federal-election-commn/,https://www.courtlistener.com/api/rest/v3/clusters/2659301/,2014,2013-033,1.0,5.0,4.0,\"There is no right more basic in our democracy than the\n",
      "right to participate in electing our political leaders.\n",
      "Cümle 1 - Lemmatized: ['category', 'cluster', 'text', 'justice', 'robert', 'majority', 'false', 'mccutcheon', 'federal', 'election', 'http', 'there', 'is', 'no', 'right', 'more', 'basic', 'in', 'our', 'democracy', 'than', 'the', 'right', 'to', 'participate', 'in', 'electing', 'our', 'political', 'leader']\n",
      "Cümle 1 - Stemmed: ['categori', 'cluster', 'text', 'justic', 'robert', 'major', 'fals', 'mccutcheon', 'feder', 'elect', 'http', 'there', 'is', 'no', 'right', 'more', 'basic', 'in', 'our', 'democraci', 'than', 'the', 'right', 'to', 'particip', 'in', 'elect', 'our', 'polit', 'leader']\n",
      "\n",
      "\n",
      "Cümle 2 - Base: Citi-\n",
      "zens can exercise that right in a variety of ways: They can\n",
      "run for office themselves, vote, urge others to vote for a\n",
      "particular candidate, volunteer to work on a campaign,\n",
      "and contribute to a candidate’s campaign.\n",
      "Cümle 2 - Lemmatized: ['zen', 'can', 'exercise', 'that', 'right', 'in', 'a', 'variety', 'of', 'way', 'they', 'can', 'run', 'for', 'office', 'themselves', 'vote', 'urge', 'others', 'to', 'vote', 'for', 'a', 'particular', 'candidate', 'volunteer', 'to', 'work', 'on', 'a', 'campaign', 'and', 'contribute', 'to', 'a', 'candidate', 's', 'campaign']\n",
      "Cümle 2 - Stemmed: ['zen', 'can', 'exercis', 'that', 'right', 'in', 'a', 'varieti', 'of', 'way', 'they', 'can', 'run', 'for', 'offic', 'themselv', 'vote', 'urg', 'other', 'to', 'vote', 'for', 'a', 'particular', 'candid', 'volunt', 'to', 'work', 'on', 'a', 'campaign', 'and', 'contribut', 'to', 'a', 'candid', 's', 'campaign']\n",
      "\n",
      "\n",
      "Cümle 3 - Base: This case is\n",
      "about the last of those options.\n",
      "Cümle 3 - Lemmatized: ['this', 'case', 'is', 'about', 'the', 'last', 'of', 'those', 'option']\n",
      "Cümle 3 - Stemmed: ['thi', 'case', 'is', 'about', 'the', 'last', 'of', 'those', 'option']\n",
      "\n",
      "\n",
      "Cümle 4 - Base: The right to participate in democracy through political\n",
      "contributions is protected by the First Amendment, but\n",
      "that right is not absolute.\n",
      "Cümle 4 - Lemmatized: ['the', 'right', 'to', 'participate', 'in', 'democracy', 'through', 'political', 'contribution', 'is', 'protected', 'by', 'the', 'first', 'amendment', 'but', 'that', 'right', 'is', 'not', 'absolute']\n",
      "Cümle 4 - Stemmed: ['the', 'right', 'to', 'particip', 'in', 'democraci', 'through', 'polit', 'contribut', 'is', 'protect', 'by', 'the', 'first', 'amend', 'but', 'that', 'right', 'is', 'not', 'absolut']\n",
      "\n",
      "\n",
      "Cümle 5 - Base: Our cases have held that Con-\n",
      "gress may regulate campaign contributions to protect\n",
      "against corruption or the appearance of corruption.\n",
      "Cümle 5 - Lemmatized: ['our', 'case', 'have', 'held', 'that', 'gress', 'may', 'regulate', 'campaign', 'contribution', 'to', 'protect', 'against', 'corruption', 'or', 'the', 'appearance', 'of', 'corruption']\n",
      "Cümle 5 - Stemmed: ['our', 'case', 'have', 'held', 'that', 'gress', 'may', 'regul', 'campaign', 'contribut', 'to', 'protect', 'against', 'corrupt', 'or', 'the', 'appear', 'of', 'corrupt']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ik 5 yazdıralım\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Base: {sentences[i]}\")\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9c4843-66f0-4749-93d1-094922af4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "file_path=os.path.join(\"stemmed_sente.csv\")\n",
    "with open(\"stemmed_sente.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    stemmed_texts=file.read()\n",
    "stemme_texts=[' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix=vectorizer.fit_transform(stemme_texts[:1000])\n",
    "features_names=vectorizer.get_feature_names_out()\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=features_names)\n",
    "with open(\"stemmee_idfff.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1faa19e1-8849-4a5a-8c54-77e002b34030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abid  abil  abl     about  abov  abridg  absenc  absent   absolut  abus  \\\n",
      "0   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "1   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "2   0.0   0.0  0.0  0.392252   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "3   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.331957   0.0   \n",
      "4   0.0   0.0  0.0  0.000000   0.0     0.0     0.0     0.0  0.000000   0.0   \n",
      "\n",
      "   ...  wrote  wyden  wyo   ye  year  yet  yield  yond  york       zen  \n",
      "0  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "1  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.222065  \n",
      "2  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "3  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "4  ...    0.0    0.0  0.0  0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "\n",
      "[5 rows x 1928 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b1be283-3146-4dde-8e7c-7c5cacd32345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. cümlede en yüksek TF-IDF skorlarına sahip 5 kelime:\n",
      "corruption    0.370780\n",
      "gress         0.351556\n",
      "regulate      0.325186\n",
      "protect       0.325186\n",
      "held          0.307817\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#ilk cümle için TF-IDF SKORLARINI AL\n",
    "first_sentence_vector=tfidf_df.iloc[4]\n",
    "#Skorlara göre sırala\n",
    "top_5_words=first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "#sonuç yazdır\n",
    "print(\"5. cümlede en yüksek TF-IDF skorlarına sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "559b6cf5-a7c3-4231-9089-4b0e11dc2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts=[' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix=vectorizer.fit_transform(lemmatized_texts[:1000])\n",
    "features_names=vectorizer.get_feature_names_out()\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=features_names)\n",
    "with open(\"lemmaaa_idfff.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00198302-67d7-4171-910c-5c3458907d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abide  ability  able     about  above  abridg  abridgement  abridgment  \\\n",
      "0    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "1    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "2    0.0      0.0   0.0  0.392252    0.0     0.0          0.0         0.0   \n",
      "3    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "4    0.0      0.0   0.0  0.000000    0.0     0.0          0.0         0.0   \n",
      "\n",
      "   absence  absent  ...  wrote  wyden  wyo  year  yes  yet  yield  yond  york  \\\n",
      "0      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "1      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "2      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "3      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "4      0.0     0.0  ...    0.0    0.0  0.0   0.0  0.0  0.0    0.0   0.0   0.0   \n",
      "\n",
      "        zen  \n",
      "0  0.000000  \n",
      "1  0.215289  \n",
      "2  0.000000  \n",
      "3  0.000000  \n",
      "4  0.000000  \n",
      "\n",
      "[5 rows x 2440 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a62935a-cddc-4166-9a4f-a9cfe1351af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "parameters=[\n",
    "    {'model_type':'cbow','window':2,'vector_size':100},\n",
    "    {'model_type':'skipgram','window':2,'vector_size':100},\n",
    "    {'model_type':'cbow','window':4,'vector_size':100},\n",
    "    {'model_type':'skipgram','window':4,'vector_size':100},\n",
    "    {'model_type':'cbow','window':2,'vector_size':300},\n",
    "    {'model_type':'skipgram','window':2,'vector_size':300},\n",
    "    {'model_type':'cbow','window':4,'vector_size':300},\n",
    "    {'model_type':'skipgram','window':4,'vector_size':300}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc495df7-33e6-4c8c-a6ae-176d0b957694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100model saved\n",
      "lemmatized_model_skipgram_window2_dim100model saved\n",
      "lemmatized_model_cbow_window4_dim100model saved\n",
      "lemmatized_model_skipgram_window4_dim100model saved\n",
      "lemmatized_model_cbow_window2_dim300model saved\n",
      "lemmatized_model_skipgram_window2_dim300model saved\n",
      "lemmatized_model_cbow_window4_dim300model saved\n",
      "lemmatized_model_skipgram_window4_dim300model saved\n",
      "stemmed_model_cbow_window2_dim100model saved\n",
      "stemmed_model_skipgram_window2_dim100model saved\n",
      "stemmed_model_cbow_window4_dim100model saved\n",
      "stemmed_model_skipgram_window4_dim100model saved\n",
      "stemmed_model_cbow_window2_dim300model saved\n",
      "stemmed_model_skipgram_window2_dim300model saved\n",
      "stemmed_model_cbow_window4_dim300model saved\n",
      "stemmed_model_skipgram_window4_dim300model saved\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model=Word2Vec(corpus, vector_size=params['vector_size'], window=params['window'], min_count=1,sg=1 if params['model_type']=='skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}model saved\")\n",
    "#lemma corpus modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "#stemma corpus modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param,\"stemmed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91ccacb8-8526-4e12-a375-fe7420b8f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#Model dosyalarını yüklemek\n",
    "model_1=Word2Vec.load(\"lemmatized_model_cbow_window2_dim100model\")\n",
    "model_2=Word2Vec.load(\"lemmatized_model_skipgram_window2_dim100model\")\n",
    "model_3=Word2Vec.load(\"lemmatized_model_cbow_window4_dim100model\")\n",
    "model_4=Word2Vec.load(\"lemmatized_model_skipgram_window4_dim100model\")\n",
    "model_5=Word2Vec.load(\"lemmatized_model_cbow_window2_dim300model\")\n",
    "model_6=Word2Vec.load(\"lemmatized_model_skipgram_window2_dim300model\")\n",
    "model_7=Word2Vec.load(\"lemmatized_model_cbow_window4_dim300model\")\n",
    "model_8=Word2Vec.load(\"lemmatized_model_skipgram_window4_dim300model\")\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity=model.wv.most_similar('protect', topn=5)\n",
    "    print(f\"\\n{model_name} Modeli - 'protect' ile benzer 5:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru:{score}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2da22479-c70e-4e6c-baee-85f7a6fb45d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lemmatized_model_cbow_window2_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.8140067458152771\n",
      "Kelime: insure, Benzerlik Skoru:0.755982518196106\n",
      "Kelime: vindicate, Benzerlik Skoru:0.7514446377754211\n",
      "Kelime: assure, Benzerlik Skoru:0.7510818243026733\n",
      "Kelime: ensure, Benzerlik Skoru:0.7506359219551086\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.7990710735321045\n",
      "Kelime: insure, Benzerlik Skoru:0.7547107338905334\n",
      "Kelime: insulate, Benzerlik Skoru:0.7509207725524902\n",
      "Kelime: protecting, Benzerlik Skoru:0.7435295581817627\n",
      "Kelime: vindicate, Benzerlik Skoru:0.7431734800338745\n",
      "\n",
      "lemmatized_model_cbow_window4_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.7885875701904297\n",
      "Kelime: secure, Benzerlik Skoru:0.7454541325569153\n",
      "Kelime: insure, Benzerlik Skoru:0.7401952743530273\n",
      "Kelime: relieve, Benzerlik Skoru:0.7338661551475525\n",
      "Kelime: assure, Benzerlik Skoru:0.7335066795349121\n",
      "\n",
      "lemmatized_model_skipgram_window4_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.7947587370872498\n",
      "Kelime: protecting, Benzerlik Skoru:0.7731378078460693\n",
      "Kelime: insure, Benzerlik Skoru:0.7458412647247314\n",
      "Kelime: assure, Benzerlik Skoru:0.7278088927268982\n",
      "Kelime: prevent, Benzerlik Skoru:0.7268767356872559\n",
      "\n",
      "lemmatized_model_cbow_window2_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.6925819516181946\n",
      "Kelime: vindicate, Benzerlik Skoru:0.6540279388427734\n",
      "Kelime: insulate, Benzerlik Skoru:0.6325324773788452\n",
      "Kelime: relieve, Benzerlik Skoru:0.6262195706367493\n",
      "Kelime: assure, Benzerlik Skoru:0.6171292662620544\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.6759305596351624\n",
      "Kelime: protecting, Benzerlik Skoru:0.6626509428024292\n",
      "Kelime: insulate, Benzerlik Skoru:0.613747239112854\n",
      "Kelime: insure, Benzerlik Skoru:0.6121201515197754\n",
      "Kelime: protects, Benzerlik Skoru:0.6072419881820679\n",
      "\n",
      "lemmatized_model_cbow_window4_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: preserve, Benzerlik Skoru:0.6856176853179932\n",
      "Kelime: protecting, Benzerlik Skoru:0.6666590571403503\n",
      "Kelime: assure, Benzerlik Skoru:0.6341134309768677\n",
      "Kelime: insure, Benzerlik Skoru:0.6325421333312988\n",
      "Kelime: prevent, Benzerlik Skoru:0.6300196647644043\n",
      "\n",
      "lemmatized_model_skipgram_window4_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: protecting, Benzerlik Skoru:0.6916472315788269\n",
      "Kelime: preserve, Benzerlik Skoru:0.6595364809036255\n",
      "Kelime: prevent, Benzerlik Skoru:0.6013708114624023\n",
      "Kelime: protects, Benzerlik Skoru:0.6005899906158447\n",
      "Kelime: insure, Benzerlik Skoru:0.5864051580429077\n"
     ]
    }
   ],
   "source": [
    "#8 model için yazır\n",
    "print_similar_words(model_1, \"lemmatized_model_cbow_window2_dim100model\")\n",
    "print_similar_words(model_2, \"lemmatized_model_skipgram_window2_dim100model\")\n",
    "print_similar_words(model_3, \"lemmatized_model_cbow_window4_dim100model\")\n",
    "print_similar_words(model_4, \"lemmatized_model_skipgram_window4_dim100model\")\n",
    "print_similar_words(model_5, \"lemmatized_model_cbow_window2_dim300model\")\n",
    "print_similar_words(model_6, \"lemmatized_model_skipgram_window2_dim300model\")\n",
    "print_similar_words(model_7, \"lemmatized_model_cbow_window4_dim300model\")\n",
    "print_similar_words(model_8, \"lemmatized_model_skipgram_window4_dim300model\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d823d73-4881-4f60-b111-06e0c2bd8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#Model dosyalarını yüklemek\n",
    "mww_1=Word2Vec.load(\"stemmed_model_cbow_window2_dim100model\")\n",
    "mww_2=Word2Vec.load(\"stemmed_model_skipgram_window2_dim100model\")\n",
    "mww_3=Word2Vec.load(\"stemmed_model_cbow_window4_dim100model\")\n",
    "mww_4=Word2Vec.load(\"stemmed_model_skipgram_window4_dim100model\")\n",
    "mww_5=Word2Vec.load(\"stemmed_model_cbow_window2_dim300model\")\n",
    "mww_6=Word2Vec.load(\"stemmed_model_skipgram_window2_dim300model\")\n",
    "mww_7=Word2Vec.load(\"stemmed_model_cbow_window4_dim300model\")\n",
    "mww_8=Word2Vec.load(\"stemmed_model_skipgram_window4_dim300model\")\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity=model.wv.most_similar('protect', topn=5)\n",
    "    print(f\"\\n{model_name} Modeli - 'protect' ile benzer 5:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "576e847d-d76d-4e97-be9d-905315cfee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stemmed_model_cbow_window2_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.7567275166511536\n",
      "Kelime: guarante, Benzerlik Skoru0.6840232610702515\n",
      "Kelime: preserv, Benzerlik Skoru0.6510797142982483\n",
      "Kelime: vindic, Benzerlik Skoru0.6398022174835205\n",
      "Kelime: secur, Benzerlik Skoru0.5963580012321472\n",
      "\n",
      "stemmed_model_skipgram_window2_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.7918824553489685\n",
      "Kelime: guarante, Benzerlik Skoru0.7174267172813416\n",
      "Kelime: tect, Benzerlik Skoru0.7038387656211853\n",
      "Kelime: vindic, Benzerlik Skoru0.6930218935012817\n",
      "Kelime: tection, Benzerlik Skoru0.671391487121582\n",
      "\n",
      "stemmed_model_cbow_window4_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.6961802840232849\n",
      "Kelime: guarante, Benzerlik Skoru0.6447535753250122\n",
      "Kelime: preserv, Benzerlik Skoru0.6039389371871948\n",
      "Kelime: vindic, Benzerlik Skoru0.5940329432487488\n",
      "Kelime: freedom, Benzerlik Skoru0.584909200668335\n",
      "\n",
      "stemmed_model_skipgram_window4_dim100model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.7611294984817505\n",
      "Kelime: guarante, Benzerlik Skoru0.7213411331176758\n",
      "Kelime: tect, Benzerlik Skoru0.6994529962539673\n",
      "Kelime: freedom, Benzerlik Skoru0.6893318295478821\n",
      "Kelime: tection, Benzerlik Skoru0.6669935584068298\n",
      "\n",
      "stemmed_model_cbow_window2_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.6414709091186523\n",
      "Kelime: preserv, Benzerlik Skoru0.5307570695877075\n",
      "Kelime: guarante, Benzerlik Skoru0.5206254124641418\n",
      "Kelime: tection, Benzerlik Skoru0.5200119018554688\n",
      "Kelime: vindic, Benzerlik Skoru0.5046795010566711\n",
      "\n",
      "stemmed_model_skipgram_window2_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.6339786648750305\n",
      "Kelime: tection, Benzerlik Skoru0.6126760840415955\n",
      "Kelime: tect, Benzerlik Skoru0.6010600328445435\n",
      "Kelime: protec, Benzerlik Skoru0.5510463714599609\n",
      "Kelime: preserv, Benzerlik Skoru0.5267605185508728\n",
      "\n",
      "stemmed_model_cbow_window4_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.6472107768058777\n",
      "Kelime: guarante, Benzerlik Skoru0.4916864335536957\n",
      "Kelime: preserv, Benzerlik Skoru0.48573538661003113\n",
      "Kelime: vindic, Benzerlik Skoru0.4751841127872467\n",
      "Kelime: tection, Benzerlik Skoru0.4552212953567505\n",
      "\n",
      "stemmed_model_skipgram_window4_dim300model Modeli - 'protect' ile benzer 5:\n",
      "Kelime: safeguard, Benzerlik Skoru0.6298116445541382\n",
      "Kelime: tect, Benzerlik Skoru0.5946353673934937\n",
      "Kelime: tection, Benzerlik Skoru0.5793569087982178\n",
      "Kelime: guarante, Benzerlik Skoru0.5656789541244507\n",
      "Kelime: protec, Benzerlik Skoru0.5290535092353821\n"
     ]
    }
   ],
   "source": [
    "#8 model için yazır\n",
    "print_similar_words(mww_1, \"stemmed_model_cbow_window2_dim100model\")\n",
    "print_similar_words(mww_2, \"stemmed_model_skipgram_window2_dim100model\")\n",
    "print_similar_words(mww_3, \"stemmed_model_cbow_window4_dim100model\")\n",
    "print_similar_words(mww_4, \"stemmed_model_skipgram_window4_dim100model\")\n",
    "print_similar_words(mww_5, \"stemmed_model_cbow_window2_dim300model\")\n",
    "print_similar_words(mww_6, \"stemmed_model_skipgram_window2_dim300model\")\n",
    "print_similar_words(mww_7, \"stemmed_model_cbow_window4_dim300model\")\n",
    "print_similar_words(mww_8, \"stemmed_model_skipgram_window4_dim300model\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91946ee3-e6d4-4805-93a0-be665076ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    ",\n",
    "from gensim.models import Word2Vec\n",
    "case[50:1000]\n",
    "#Model dosyalarını yüklemek\n",
    "mww_1=Word2Vec.load(\"stemmed_model_cbow_window2_dim100.model\")\n",
    "mww_2=Word2Vec.load(\"stemmed_model_skipgram_window2_dim100.model\")\n",
    "mww_3=Word2Vec.load(\"stemmed_model_cbow_window4_dim100.mdoel\")\n",
    "mww_4=Word2Vec.load(\"stemmed_model_skipgram_window4_dim100.model\")\n",
    "mww_5=Word2Vec.load(\"stemmed_model_cbow_window2_dim300.model\")\n",
    "mww_6=Word2Vec.load(\"stemmed_model_skipgram_window2_dim300.model\")\n",
    "mww_7=Word2Vec.load(\"stemmed_model_cbow_window3_dim300.model\")\n",
    "mww_8=Word2Vec.load(\"stemmed_model_skipgram_window3_dim300.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
