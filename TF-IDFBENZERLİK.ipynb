{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079b53f-ddb5-41c3-8f98-df51fa13e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be04d0-a475-4bad-b5ca-6b6d906c3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(\"opinions.csv\")\n",
    "with open(\"opinions.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    case=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda403b4-1f3a-44ea-8ea6-630acdf8e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words_list=list(stop_words)\n",
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881b38f-c596-46e3-96e7-0eaa7dc2a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bir bölümü\n",
    "case[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f0c5f-492b-45c0-9fa7-837e59772e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize ve stemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stemmer=PorterStemmer()\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28c3dc-09eb-4d07-9897-bf3eea822392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d17a86-5c66-40c1-881f-d7345fc0cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    filtered_tokens=[token.lower() for token in tokens if token.isalpha() and token.lower not in stop_words]\n",
    "\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c4700-61f7-4b1d-9bd8-b094e5f368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#her cünleyi tokenleştir ve lemmatize et ve stemmle\n",
    "tokenized_corpus_lemmatized=[]\n",
    "tokenized_corpus_stemmed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518f1b3-6467-401b-b34b-9e2895b54cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens=preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f93bfe-6256-4b3f-8391-9aeb40184af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemmatized_sente.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6e360-48e0-4311-a667-2e44223fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stemmed_sente.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490ee11-44d9-427e-8b5b-ae228f1dd808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ik 5 yazdıralım\n",
    "for i in range(5):\n",
    "    print(f\"Cümle {i+1} - Base: {sentences[i]}\")\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c4843-66f0-4749-93d1-094922af4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "file_path=os.path.join(\"stemmed_sente.csv\")\n",
    "with open(\"stemmed_sente.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    stemmed_texts=file.read()\n",
    "stemme_texts=[' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix=vectorizer.fit_transform(stemme_texts[:1000])\n",
    "features_names=vectorizer.get_feature_names_out()\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=features_names)\n",
    "with open(\"stemmee_idfff.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa19e1-8849-4a5a-8c54-77e002b34030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1be283-3146-4dde-8e7c-7c5cacd32345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ilk cümle için TF-IDF SKORLARINI AL\n",
    "first_sentence_vector=tfidf_df.iloc[4]\n",
    "#Skorlara göre sırala\n",
    "top_5_words=first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "#sonuç yazdır\n",
    "print(\"ilk cümlede en yüksek TF-IDF skorlarına sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b6cf5-a7c3-4231-9089-4b0e11dc2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts=[' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix=vectorizer.fit_transform(lemmatized_texts[:1000])\n",
    "features_names=vectorizer.get_feature_names_out()\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=features_names)\n",
    "with open(\"lemmaaa_idfff.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    #her cümleyi ayrı satır olarak yaz\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00198302-67d7-4171-910c-5c3458907d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91946ee3-e6d4-4805-93a0-be665076ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from  sklearn.metrics.pairwise import cosine_similarity\n",
    "#regulate kelimesinin vektörü\n",
    "regulate_index=features_names.tolist().index('regulate')\n",
    "regulate_vector=tfidf_matrix[:, regulate_index]. toarray()\n",
    "tfidf_vectors=tfidf_matrix.toarray()\n",
    "similarities=cosine_similarity(regulate_vector.T, tfidf_vectors.T)\n",
    "similarities=similarities.flatten()\n",
    "top_5_indices=similarities.argsort()[-6:][::-1]\n",
    "for index in top_5_indices:\n",
    "    print(f\"{features_names[index]}:{similarities[index]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8365304-6b04-4699-8fb4-61975a2425c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
